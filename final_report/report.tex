\documentclass{llncs}
\usepackage{setspace}
\usepackage{caption}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{url}
\usepackage{todonotes}
\usepackage{listings}
\linespread{1.3}

\title{Malware Classification Project Report}

\author{Andrea Mambretti}
\institute{Northeastern University\\
College of Computer and Information Science\\
Boston, 20115, USA \\
\email{mbr@ccs.neu.edu}}

\begin{document}
\flushbottom
\maketitle

\section*{Problem definition}

In recent years, the malware growth and distribution reached unthinkable levels.
A 2015 report\cite{trendmicro} from TrendMicro reveals that almost 1 million
samples are discovered on daily base. Given these numbers, it is easy to realize
that a manual approach to handle this immense flow of new threats is absolutely
unbearable. Therefore, since beginning of 2000 researcher have been trying to
apply automatic techniques to help to catch up with this huge growth.

Several approaches has been proposed, covering the huge spectrum that goes from
static to dynamic analysis. In malware analysis, static approaches tend
to suffer because packing and encryption. Malware authors employ these techniques
to prevent fast and reliable reverse engineering and or automatic analysis on
the malware binary. On the other hand, dynamic analysis is instead
able to deal with these techniques but suffer of code coverage issues. Most of
the time malware relies on specific inputs or time conditions to trigger
the malice behaviours.

Given the complexity of deployment of most of the dynamic or static analysis
techniques, researchers have also tried to apply machine learning techniques to
the process to easier the analysis and classification of the samples. In this
case, researchers exploited the key idea that most of the samples discovered has
commonalities with previous discovered samples. Machine learning can help to
pre-filter malware based on certain features and classify them in the respective
family leaving only a smaller subset of unknown malware to be analyzed to the
analysts.


In general, it is fair to assume that the number of malware families is largely
smaller compared the number of samples discovered. Most of the new samples
represents updated version of what was already known in the past. This offer
good ground for machine learning approaches.Previously, machine learning
approaches such as \cite{torsten} applied classification on malware. Others,
such as \cite{unsupmaldetection} applied un-supervised learning methodology to cluster
similar malware without having previous ground truth. All these works
concentrate mostly on dynamic features due to the encryption and packing.


In this project, I tackle the problem of malware classification using a complete
set of static features. This will allow a fast and inexpensive analysis to
pre-filter malware in the respective families. For this work I use the publicly
available Microsoft Kaggle Database which offers a large collection (10800
samples) of malware divided in 9 families. For each sample, the database contains
the bytes and the IDApro disassembled view of the malware.


\section*{Dataset Description}

In this work, I used the publicly available malware dataset offered by Microsoft
for the malware classification challenge competition held between February and
April 2015. It contains 9 different families of malware showed in Table
\ref{tab:samples}. The dataset contains a total of 10868 samples. The dataset
contains also other 17GB of data which goal was to test the various algorithms
during the competition. I do not use that part of the dataset in this project
since unlabeled.

The dataset offers for each sample two distinct files. It contains a
\textit{bytes} file which provides the hexdump of the sample (without the PE
header) and a IDApro disassembled file which contains the assembly code in Intel
syntax and other useful information (e.g. section names, sub routine start and
end etc.). Every sample in the database is not encrypted and not packed. I
explicitly exploit this characteristic of the dataset to extract static features
that otherwise I would not be able to use. I do not extract any dynamic features
(e.g. time of execution or operation triggered by certain inputs) because every
sample misses the PE header preventing me its execution.

Based on the Microsoft Malware Classification Challenge website, the database
contains malware discovered by Microsoft's real-time detection anti-malware
products on over 160M computers worldwide inspecting 700M computers monthly.
This dataset represents a very unique point of start for machine learning
experiments due to its nature and how the data were collected. All previous
works in this field had common issues in their evaluation due to the scarcity of
well labeled enough big datasets.


\begin{table}
    \begin{center}
        \begin{tabular}{ |c|c|c| }
        \hline
        \textbf{Num}&\textbf{Family}&\textbf{\# Samples}\\
        \hline
        1 &Ramnit&1541\\
        \hline
        2 &Lollipop&2478\\
        \hline
        3 &Kelihos\_ver3&2942\\
        \hline
        4 &Vundo&475\\
        \hline
        5 &Simda&42\\
        \hline
        6 &Tracur&751\\
        \hline
        7 &Kelihos\_ver1&398\\
        \hline
        8 &Obfuscator.ACY&1228\\
        \hline
        9 &Gatak&1013\\
        \hline
        \end{tabular}
    \captionsetup{width=0.5\textwidth, skip=10pt}
    \caption{This table shows the number of samples per families
    in the training Microsoft Malware Classification challenge dataset}
    \label{tab:samples}
    \end{center}
\end{table}


\section*{Proposed Solution}
\subsection*{Feature Selection}
Given the dataset and its characteristics described in the previous paragraph, I
decided to approach this classification problem trying to define the best
features set. The first common element I found looking at the malware of the same
family is the size (in bytes) of the malware. The size generally changes of few
KB at the maximum in case of samples from the same family meanwhile among
families this value changes up to an order of magnitude. A second feature that I
discovered to be very specific of the malware family is the strings (sequence of
printable characters within the file).

The other features I took into account are related to information contained in
the IDApro file provided. More specifically, I counted the number of occurrences
of certain common assembly operations such as movs, jmps, calls. Furthermore, I
used as features the sections used within the binary (e.g. .text, .idata, .rdata
etc.). Some of these section are actually common but others are very compiler
and program specific therefore they tend to characterize file compiled with the
same compiler and that shares certain information. Finally,  I counted the numer
of sub routines for each sample. Also this number is characteristic of the
functions implemented within each sample and I would expect this value to change
only slightly among samples of the same family.

\begin{table}
    \begin{center}
        \begin{tabular} {|c|c|c|}
            \hline
            \textbf{Num}&\textbf{Feature}&\textbf{Type} \\
            \hline
            1 &Strings&Sparse boolean vector \\
            \hline
            2 &Sections&Sparse boolean vector \\
            \hline
            3 &File Size&Integer \\
            \hline
            4 &Subroutine Count&Integer \\
            \hline
            5 &Calls op. Count&Integer \\
            \hline
            6 &Mov op. Count&Integer \\
            \hline
            7 &Jmps op. Count&Integer \\
            \hline
            8 &Pop op. Count&Integer \\
            \hline
            9 &Push op. Count&Integer \\
            \hline
            10 &Xor op. Count&Integer \\
            \hline
            11 &Sub op. Count&Integer \\
            \hline
            12 &Add op. Count&Integer \\
            \hline
        \end{tabular}
        \captionsetup{width=0.5\textwidth, skip=10pt}
        \caption{Features collected for each sample}
    \end{center}
\end{table}

After the collection of all the strings and sections and their conversion to
sparser boolean vectors, the features vector for each samples counts a total of
410318 features.

\subsection*{Machine Learning Algorithms}

The problem that I'm trying to solve with this project requires multi-class
algorithm classifier. Based on the dataset, I have 9 different labels which
translates on 9 different classes of malware that I aim to classifly. Among the
algorithms that support multi-class classification, I choose the following three:
\begin{itemize}
    \item{Support Vector Machines (SVM)}
    \item{Random Forest}
    \item{Neural Network}
\end{itemize}
One of the goals of the project is comparing the performance and
accuracy of the above listed classification methodology. To do so, I trained
multiple times, with various settings, each of the classifiers trying to find the
best configuration for the features selected. The research of the best setting
was made easier thanks to the Grid Research offered by scikit-learn python
package which given a dictionary of possible options perform the training for
all the combinations among the specified parameters. In the result section, I
will show the best settings and the results comparison for the algorithms. For
the research of the best settings, I divided the training set in two, I used
33.3\% of the dataset for testing and the remaining 66.6\% for the training.

\subsection*{Challenges}
During the collection of the features, I encountered various challenges mostly
related to the size of the dataset. In this section, I will try to list the most
meaningful ones.

The first issue I came across is extracting in a proper way the string
within the hexdump. The main operations involved for the extraction are string
replacement and then 2bytes x 2bytes conversion to ascii characters and
verification if they belongs to the printable characters. I had very poor
performance with both pypy and python such that the application of all the
dataset would have required more than a day. A deeper analysis of the algorithm
showed that the first operation was faster in python (about 10x faster than
pypy) meanwhile the second operation was a way faster in pypy (8x faster than
python). Therefore, based on this observation, I decided to split the script in
two pieces and the overall execution time came down to 30 minutes on the whole
dataset.

\todo[inline]{Write some more issure during the implementation}

\section*{Results}
In this section, I present the results of the project. For each classifier, I
show the various configuration tried and which one is providing me the best
performance. Lastly, I present a comparison among the various algorithms used.
During the research for the best configuration the dataset is divided in 33\%
test set and 66\% training set. Meanwhile, the test for comparison among the
best configuration for each classifier uses 10K-fold for cross-validating the
results.

\subsection*{Support Vector Machine}
During the research of best configuration, I \textit{brute-forced} some of the
parameters. Table \ref{tab:svm_param} shows exactly which parameters were
tried for SVM.

\begin{table}
    \begin{center}
        \begin{tabular}{|c|c|}
            \hline
            \textbf{Parameter} & \textbf{Values} \\
            \hline
            kernel&rbf, linear \\
            \hline
            gamma& 1e-3, 3e-4 \\
            \hline
            C& 1, 10, 100, 1000 \\
            \hline
        \end{tabular}
        \captionsetup{width=0.5\textwidth, skip=10pt}
        \caption{Settings tried for SVM best setting research}
        \label{tab:svm_param}
    \end{center}
\end{table}

As result of this search, I found that  the best configuration is
\begin{lstlisting}
`kernel' : `rbf',
`C' : 100,
`gamma' : 0.0001
\end{lstlisting}

In the appendix \ref{appendix_a}, I show the score given  to also the others configuration tried.
Hereafter, I report the results on the test set for this configuration.

\begin{table}
    \begin{center}
        \begin{tabular} {|c|c|c|c|c|}
        \hline
        \textbf{Class}&\textbf{Precision}&\textbf{Recall}&\textbf{F1-score}&\textbf{Support} \\
        \hline
        1&0.98&0.99&0.98&518 \\
        \hline
        2&1.00&1.00&1.00&844 \\
        \hline
        3&1.00&1.00&1.00&956 \\
        \hline
        4&0.90&0.98&0.94&145 \\
        \hline
        5&1.00&0.87&0.93&15 \\
        \hline
        6&0.98&0.99&0.98&248 \\
        \hline
        7&1.00&0.97&0.99&139 \\
        \hline
        8&0.98&0.96&0.97&400 \\
        \hline
        9&0.99&1.00&1.00&322 \\
        \hline
        Tot&0.99&0.99&0.99&3587 \\
        \hline
        \end{tabular}
        \captionsetup{width=0.5\textwidth, skip=10pt}
        \caption{Results of the SVM best settings on the test dataset}
    \end{center}
\end{table}
\subsection*{Random Forest}
In the Random Forest case, I \textit{brute-foced} the parameters in Table
\ref{tab:random_forest_param}.
\begin{table}
    \begin{center}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Parameter} & \textbf{Values} \\
        \hline
        max\_depth & 3, None \\
        \hline
        min\_samples\_split & 1,3,10 \\
        \hline
        min\_samples\_leaf& 2,3,10 \\
        \hline
        bootstrap & True, False \\
        \hline
        criterion & gini, entroy \\
        \hline
    \end{tabular}
    \captionsetup{width=0.5\textwidth, skip=10pt}
    \caption{Settings tried for Random Forest best settings research}
    \label{tab:random_forest_param}
    \end{center}
\end{table}

As result of this search, I found that the best configuration is
\begin{lstlisting}
`bootstrap': False,
`min_samples_leaf':1,
`min_samples_split':2,
`criterion':`gini',
`max_features': 3,
`max_depth': None
\end{lstlisting}

In Table \ref{tab:random_forest_res_bp}, I report the results on the test set
for this configuration.

\begin{table}
    \begin{center}
        \begin{tabular} {|c|c|c|c|c|}
        \hline
        \textbf{Class}&\textbf{Precision}&\textbf{Recall}&\textbf{F1-score}&\textbf{Support} \\
        \hline
        1&0.93&0.98&0.96&518 \\
        \hline
        2&0.99&0.99&0.99&844 \\
        \hline
        3&1.00&0.99&1.00&956 \\
        \hline
        4&0.90&0.98&0.94&145 \\
        \hline
        5&0.90&0.60&0.72&15 \\
        \hline
        6&1.00&0.99&0.97&248 \\
        \hline
        7&0.98&0.96&0.97&139 \\
        \hline
        8&0.98&0.92&0.95&400 \\
        \hline
        9&0.99&1.00&0.99&322 \\
        \hline
        Tot&0.98&0.98&0.98&3587 \\
        \hline
        \end{tabular}
        \captionsetup{width=0.5\textwidth, skip=10pt}
        \caption{Results of the Random Forest best settings on the test dataset}
        \label{tab:random_forest_res_bp}
    \end{center}
\end{table}

\subsection*{Neural Network}

For Neural Networks, Table \ref{tab:nn_param} shows which combinations of
parameters I tried.

\begin{table}
    \begin{center}
        \begin{tabular}{|c|c|}
            \hline
            \textbf{Parameters} & \textbf{Values} \\
            \hline
            `Solver' & lbfgs, sgd, adam \\
            \hline
            `Alpha' & 0.0001, 0.001, 0.01, 0.1, 0.9 \\
            \hline
            `Activation' & Identity, Logistic, Tanh, Relu \\
            \hline
            `Learning\_rate' & Constant, Invscaling, Adaptive \\
            \hline
        \end{tabular}
        \captionsetup{width=0.5\textwidth, skip=10pt}
        \caption{Settings tried for Neural Network best settings research}
        \label{tab:nn_param}
    \end{center}
\end{table}

As result of this search, I found that the best configuration is
\begin{lstlisting}
`alpha': 0.01,
`activation':`identity',
`learning_rate':`adaptive',
`solver'= 'adam`
\end{lstlisting}

In Table \ref{tab:nn_res_bp}, I report the results on the test set for this
configuration.

\begin{table}
    \begin{center}
        \begin{tabular} {|c|c|c|c|c|}
        \hline
        \textbf{Class}&\textbf{Precision}&\textbf{Recall}&\textbf{F1-score}&\textbf{Support} \\
        \hline
        1&0.97&0.95&0.96&518 \\
        \hline
        2&0.99&0.99&0.99&844 \\
        \hline
        3&1.00&0.99&0.99&956 \\
        \hline
        4&0.79&0.98&0.87&145 \\
        \hline
        5&0.00&0.00&0.00&15 \\
        \hline
        6&0.97&0.98&0.98&248 \\
        \hline
        7&0.96&0.95&0.95&139 \\
        \hline
        8&0.99&0.92&0.95&400 \\
        \hline
        9&0.93&0.98&0.95&322 \\
        \hline
        Tot&0.97&0.97&0.97&3587 \\
        \hline
        \end{tabular}
        \captionsetup{width=0.5\textwidth, skip=10pt}
        \caption{Results of the Neural Network best settings on the test dataset}
        \label{tab:nn_res_bp}
    \end{center}
\end{table}

\section*{Related Work}

\section*{Future Work}

\section*{Conclusion}

\section*{Appendix A} \label{appendix_a}
\newpage
\bibliography{mybib}{}
\bibliographystyle{plain}
\end{document}

