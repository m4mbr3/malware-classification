\documentclass{llncs}
\usepackage{setspace}
\usepackage{caption}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{url}
\linespread{1.3}

\title{Malware Classification Project Report}

\author{Andrea Mambretti}
\institute{Northeastern University\\
College of Computer and Information Science\\
Boston, 20115, USA \\
\email{mbr@ccs.neu.edu}}

\begin{document}
\flushbottom
\maketitle

\section*{Problem definition}

In recent years, the malware growth and distribution reached unthinkable levels.
A 2015 report\cite{trendmicro} from TrendMicro reveals that almost 1 million
samples are discovered on daily base. Given these numbers, it is easy to realize
that a manual approach to handle this immense flow of new threats is absolutely
unbearable. Therefore, since beginning of 2000 researcher have been trying to
apply automatic techniques to help to catch up with this huge growth.

Several approaches has been proposed, covering the huge spectrum that goes from
static to dynamic analysis. In malware analysis, static approaches tend
to suffer because packing and encryption. Malware authors employ these techniques
to prevent fast and reliable reverse engineering and or automatic analysis on
the malware binary. On the other hand, dynamic analysis is instead
able to deal with these techniques but suffer of code coverage issues. Most of
the time malware relies on specific inputs or time conditions to trigger
the malice behaviours.

Given the complexity of deployment of most of the dynamic or static analysis
techniques, researchers have also tried to apply machine learning techniques to
the process to easier the analysis and classification of the samples. In this
case, researchers exploited the key idea that most of the samples discovered has
commonalities with previous discovered samples. Machine learning can help to
pre-filter malware based on certain features and classify them in the respective
family leaving only a smaller subset of unknown malware to be analyzed to the
analysts.


In general, it is fair to assume that the number of malware families is largely
smaller compared the number of samples discovered. Most of the new samples
represents updated version of what was already known in the past. This offer
good ground for machine learning approaches.Previously, machine learning
approaches such as \cite{torsten} applied classification on malware. Others,
such as \cite{unsupmaldetection} applied un-supervised learning methodology to cluster
similar malware without having previous ground truth. All these works
concentrate mostly on dynamic features due to the encryption and packing.


In this project, I tackle the problem of malware classification using a complete
set of static features. This will allow a fast and inexpensive analysis to
pre-filter malware in the respective families. For this work I use the publicly
available Microsoft Kaggle Database which offers a large collection (10800
samples) of malware divided in 9 families. For each sample, the database contains
the bytes and the IDApro disassembled view of the malware.


\section*{Dataset Description}

In this work, I used the publicly available malware dataset offered by Microsoft
for the malware classification challenge competition held between February and
April 2015. It contains 9 different families of malware showed in Table
\ref{tab:samples}. The dataset contains a total of 10868 samples. The dataset
contains also other 17GB of data which goal was to test the various algorithms
during the competition. I do not use that part of the dataset in this project
since unlabeled.

The dataset offers for each sample two distinct files. It contains a
\textit{bytes} file which provides the hexdump of the sample (without the PE
header) and a IDApro disassembled file which contains the assembly code in Intel
syntax and other useful information (e.g. section names, sub routine start and
end etc.). Every sample in the database is not encrypted and not packed. I
explicitly exploit this characteristic of the dataset to extract static features
that otherwise I would not be able to use. I do not extract any dynamic features
(e.g. time of execution or operation triggered by certain inputs) because every
sample misses the PE header preventing me its execution.

Based on the Microsoft Malware Classification Challenge website, the database
contains malware discovered by Microsoft's real-time detection anti-malware
products on over 160M computers worldwide inspecting 700M computers monthly.
This dataset represents a very unique point of start for machine learning
experiments due to its nature and how the data were collected. All previous
works in this field had common issues in their evaluation due to the scarcity of
well labeled enough big datasets.


\begin{table}
    \begin{center}
        \begin{tabular}{ |c|c|c| }
        \hline
        \textbf{Num}&\textbf{Family}&\textbf{\# Samples}\\
        \hline
        1 &Ramnit&1541\\
        \hline
        2 &Lollipop&2478\\
        \hline
        3 &Kelihos\_ver3&2942\\
        \hline
        4 &Vundo&475\\
        \hline
        5 &Simda&42\\
        \hline
        6 &Tracur&751\\
        \hline
        7 &Kelihos\_ver1&398\\
        \hline
        8 &Obfuscator.ACY&1228\\
        \hline
        9 &Gatak&1013\\
        \hline
        \end{tabular}
    \captionsetup{width=0.5\textwidth, skip=10pt}
    \caption{This table shows the number of samples per families
    in the training Microsoft Malware Classification challenge dataset}
    \label{tab:samples}
    \end{center}
\end{table}


\section*{Proposed Solution}
\subsection*{Feature Selection}
Given the dataset and its characteristics described in the previous paragraph, I
decided to approach this classification problem trying to define the best
features set. The first common element I found looking at the malware of the same
family is the size (in bytes) of the malware. The size generally changes of few
KB at the maximum in case of samples from the same family meanwhile among
families this value changes up to an order of magnitude. A second feature that I
discovered to be very specific of the malware family is the strings (sequence of
printable characters within the file).

The other features I took into account are related to information contained in
the IDApro file provided. More specifically, I counted the number of occurrences
of certain common assembly operations such as movs, jmps, calls. Furthermore, I
used as features the sections used within the binary (e.g. .text, .idata, .rdata
etc.). Some of these section are actually common but others are very compiler
and program specific therefore they tend to characterize file compiled with the
same compiler and that shares certain information. Finally,  I counted the numer
of sub routines for each sample. Also this number is characteristic of the
functions implemented within each sample and I would expect this value to change
only slightly among samples of the same family.

\begin{table}
    \begin{center}
        \begin{tabular} {|c|c|c|}
            \hline
            \textbf{Num}&\textbf{Feature}&\textbf{Type} \\
            \hline
            1 &Strings&Sparse boolean vector \\
            \hline
            2 &Sections&Sparse boolean vector \\
            \hline
            3 &File Size&Integer \\
            \hline
            4 &Subroutine Count&Integer \\
            \hline
            5 &Calls op. Count&Integer \\
            \hline
            6 &Mov op. Count&Integer \\
            \hline
            7 &Jmps op. Count&Integer \\
            \hline
            8 &Pop op. Count&Integer \\
            \hline
            9 &Push op. Count&Integer \\
            \hline
            10 &Xor op. Count&Integer \\
            \hline
            11 &Sub op. Count&Integer \\
            \hline
            12 &Add op. Count&Integer \\
            \hline
        \end{tabular}
        \captionsetup{width=0.5\textwidth, skip=10pt}
        \caption{Features collected for each sample}
    \end{center}
\end{table}

After the collection of all the strings and sections and their conversion to
sparser boolean vectors, the features vector for each samples counts a total of
410318 features.

\subsection*{Machine Learning Algorithms}

The problem that I'm trying to solve with this project requires multi-class
algorithm classifier. Based on the dataset, I have 9 different labels which
translates on 9 different classes of malware that I aim to classifly. Among the
algorithms that support multi-class classification, I choose the following three:
\begin{itemize}
    \item{Support Vector Machines (SVM)}
    \item{Random Forest}
    \item{Neural Network}
\end{itemize}
One of the goals of the project is comparing the performance and
accuracy of the above listed classification methodology. To do so, I trained
multiple times, with various settings, each of the classifiers trying to find the
best configuration for the features selected. The research of the best setting
was made easier thanks to the Grid Research offered by scikit-learn python
package which given a dictionary of possible options perform the training for
all the combinations among the specified parameters. In the result section, I
will show the best settings and the results comparison for the algorithms. For
the research of the best settings, I divided the training set in two, I used
33.3\% of the dataset for testing and the remaining 66.6\% for the training.

\subsection*{Challenges}
During the collection of the features, I encountered various challenges mostly
related to the size of the dataset. In this section, I will try to list the most
meaningful ones.

\bibliography{mybib}{}
\bibliographystyle{plain}
\end{document}

