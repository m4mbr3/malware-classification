import os
import sys
import time
import json
import pickle
import progressbar

import numpy as np

from scipy.sparse import csr_matrix
from scipy.sparse import vstack
from scipy import io

from sklearn import svm

from sklearn.externals import joblib

from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split

from sklearn.metrics import classification_report
from sklearn.metrics import coverage_error

from multiprocessing import Pool
from multiprocessing import Lock

"""DEBUG print switch"""
DEBUG=False
"""absolute DIR of the program"""
DIR  = os.path.abspath(sys.argv[1])
"""Contains the dataset"""
matrix = csr_matrix([])
"""Status bar"""
bar = None
"""Counter for status bar"""
counter = 0
"""Features labels"""
y = []
"""Lock"""
lock = Lock()
"""sample"""
y_sample = [[1,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0,0],[0,0,1,0,0,0,0,0,0],[0,0,0,1,0,0,0,0,0],[0,0,0,0,1,0,0,0,0],[0,0,0,0,0,1,0,0,0],[0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,0,1]]

"""Callback function"""
def update(r):
    global y
    global matrix
    global lock
    global counter
    vect, feat = r
    lock.acquire()
    y.append(feat)

    if matrix.getnnz() == 0:
        matrix = csr_matrix(vect)
    else:
        matrix = vstack([matrix,vect])
    counter += 1
    lock.release()
    bar.update(counter)

"""LoadJSON"""
def loadJSON(f, worker_id):
    with open (DIR+ "/" + f, "r") as fd:
        data = json.load(fd)
        y = data['family']

        """Deleting useless key"""
        del data['bytes_path']
        del data['family']
        del data['asm_path']
        del data['id']

    return (data.values(),y)

pool = Pool(processes=20)


"""Sfm trainer main function"""
def loadingData():
    global bar
    global matrix
    global y
    global pool

    i = 0
    for root, dirs, filenames in os.walk(DIR):
        for f in filter(lambda x:x.endswith(".json"), filenames):
            pool.apply_async(loadJSON,
                             args=(f,i,),
                             callback=update)
            i += 1
        bar = progressbar.ProgressBar(max_value=i)
        pool.close()
        pool.join()

    io.mmwrite('matrix.mtx', matrix)

    with open('list.mtx', 'w') as file_:
        for item in y:
            file_.write("{}\n".format(item))

"""Neural Network"""
def nn_trainer(X_train, X_test, y_train, y_test):
    tuned_parameters = [
        {
            'solver':['lbfgs'],
            'alpha': [0.0001, 0.001, 0.01, 0.1, 0.9],
            'activation':['identity','logistic','tanh','relu'],
            'learning_rate': ['constant','invscaling','adaptive']
        },
        {
            'solver':['sgd'],
            'alpha': [0.0001, 0.001, 0.01, 0.1, 0.9],
            'activation':['identity','logistic','tanh','relu'],
            'learning_rate': ['constant','invscaling','adaptive']
        },
        {
            'solver': ['adam'],
            'alpha': [0.0001, 0.001, 0.01, 0.1, 0.9],
            'activation':['identity','logistic','tanh','relu'],
            'learning_rate': ['constant','invscaling','adaptive']
        }]
    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
                        hidden_layer_sizes=(5,2), random_state=1)
    clf = GridSearchCV(clf, param_grid=tuned_parameters, n_jobs=-1)
    start = time.time()

    clf.fit(X_train, y_train)

    joblib.dump(clf,'nn_trainer_grid.pkl')

    print("GridSearchCV took %.2f seconds for %d candidate parameter settings."
                % (time.time() - start, len(clf.cv_results_['params'])))

    report(clf.cv_results_)

    print("Best parameters set found on development set:")
    print()
    print(clf.best_params_)
    print()
    print("Grid scores on development set:")
    print()
    means = clf.cv_results_['mean_test_score']
    stds = clf.cv_results_['std_test_score']
    for mean, std, params in zip(means, stds, clf.cv_results_['params']):
        print("%0.3f (+/-%0.03f) for %r"
            % (mean, std * 2, params))
    print()

    print("Detailed classification report:")
    print()
    print("The model is trained on the full development set.")
    print("The scores are computed on the full evaluation set.")
    print()
    y_true, y_pred = y_test, clf.predict(X_test)
    print(classification_report(y_true, y_pred))
    print()

    # clf = clf.fit(X_train, y_train)

    # joblib.dump(clf, 'nn_model.pkl')

    # s = []

    # for i, row in enumerate(X_test):
        # s.append(coverage_error([y_sample[y_test[i]-1]],
                                  # clf.predict_proba(row)))
    # i = 0
    # pos = 0
    # neg = 0
    # for res in clf.predict(X_test):
        # if res == y[i]:
            # pos +=1
        # else:
            # neg +=1
        # i+=1
    # print "Tot {}\nPos {}\nNeg {}".format(pos+neg, pos, neg)

    # pickle.dump(s, open('sum_nn.txt','w'))

# Utility function to report best scores
def report(results, n_top=3):
    for i in range(1, n_top + 1):
        candidates = np.flatnonzero(results['rank_test_score'] == i)
        for candidate in candidates:
            print("Model with rank: {0}".format(i))
            print("Mean validation score: {0:.3f} (std: {1:.3f})".format(
                  results['mean_test_score'][candidate],
                  results['std_test_score'][candidate]))
            print("Parameters: {0}".format(results['params'][candidate]))
            print("")

"""Random forste function"""
def rf_trainer(X_train, X_test, y_train, y_test):
    clf = RandomForestClassifier(n_estimators=20)

    param_grid = {"max_depth": [3, None],
              "max_features": [1, 3, 10],
              "min_samples_split": [2, 3, 10],
              "min_samples_leaf": [1, 3, 10],
              "bootstrap": [True, False],
              "criterion": ["gini", "entropy"]}

    clf = GridSearchCV(clf, param_grid=param_grid, n_jobs=-1)

    start = time.time()

    clf.fit(X_train, y_train)

    joblib.dump(clf,'rf_trainer_grid.pkl')

    print("GridSearchCV took %.2f seconds for %d candidate parameter settings."
                % (time.time() - start, len(clf.cv_results_['params'])))

    report(clf.cv_results_)

    print("Best parameters set found on development set:")
    print()
    print(clf.best_params_)
    print()
    print("Grid scores on development set:")
    print()
    means = clf.cv_results_['mean_test_score']
    stds = clf.cv_results_['std_test_score']
    for mean, std, params in zip(means, stds, clf.cv_results_['params']):
        print("%0.3f (+/-%0.03f) for %r"
            % (mean, std * 2, params))
    print()

    print("Detailed classification report:")
    print()
    print("The model is trained on the full development set.")
    print("The scores are computed on the full evaluation set.")
    print()
    y_true, y_pred = y_test, clf.predict(X_test)
    print(classification_report(y_true, y_pred))
    print()




"""SVM trainer function"""
def svm_trainer(X_train, X_test, y_train, y_test):
    tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                     'C': [1, 10, 100, 1000]},
                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]

    scores = ['precision', 'recall']
    i = 0
    for score in scores:
        print("# Tuning hyper-parameters for %s" % score)
        print()

        clf = GridSearchCV(svm.SVC(C=1), tuned_parameters, cv=5,
                        scoring='%s_macro' % score, n_jobs=-1)
        clf.fit(X_train, y_train)
        joblib.dump(clf, 'svm_model{}.pkl'.format(i))
        i += 1
        print("Best parameters set found on development set:")
        print()
        print(clf.best_params_)
        print()
        print("Grid scores on development set:")
        print()
        means = clf.cv_results_['mean_test_score']
        stds = clf.cv_results_['std_test_score']
        for mean, std, params in zip(means, stds, clf.cv_results_['params']):
            print("%0.3f (+/-%0.03f) for %r"
                % (mean, std * 2, params))
        print()

        print("Detailed classification report:")
        print()
        print("The model is trained on the full development set.")
        print("The scores are computed on the full evaluation set.")
        print()
        y_true, y_pred = y_test, clf.predict(X_test)
        print(classification_report(y_true, y_pred))
        print()


def trainer():
    global matrix
    global y
    if matrix.getnnz() == 0:
        matrix = io.mmread('matrix.mtx')

    if len(y) == 0:
        with open('list.mtx', 'r') as file_:
            for l in file_:
                y.append(int(l))

    X_train, X_test, y_train, y_test = train_test_split (matrix, y,
                                                         test_size=0.33,
                                                         random_state=55)

    # svm_trainer(X_train, X_test, y_train, y_test)
    # rf_trainer(X_train, X_test, y_train, y_test)
    nn_trainer(X_train, X_test, y_train, y_test)


if __name__ == "__main__":
    if not ((os.path.isfile('matrix.mtx') and os.path.isfile('list.mtx'))):
        loadingData()
    trainer()
