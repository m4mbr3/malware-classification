import os
import sys
import json
import timeit
import traceback
import progressbar

import numpy as np

from pprint import pprint
from multiprocessing import Pool

# from malclass import element
# from malclass import program_information

from sklearn import preprocessing
# from sklearn.feature_extraction import DictVectorizer


"""Absolute DIR of the program"""
DIR = os.path.abspath(sys.argv[1])
DIR_NORM = DIR+"/../normalyzed"
try:
    os.makedirs(DIR_NORM)
except OSError:
    print 'Folder {} already created'.format(DIR_NORM)

"""Counter of file analyzed"""
counter = 0
"""Debug prints switch"""
DEBUG = False
"""Features"""
features = ["size","bb","family","call","mov","push","pop","add","sub","xor","jmp"]
"""Dataset matrix"""
matrix = None
"""Bar for status"""
bar = None
"""String Set"""
collected_strings = dict()
"""section"""
collected_section = dict()
"""String dictionary"""
coll_strings_dict = None
"""section dictionary"""
coll_section = None

"""
Call back function for multiprocessing
"""
def update(r):
    global counter
    global bar
    counter += 1
    bar.update(counter)

"""
JSON db loader
f: file descriptor from open
"""
def loadJSON(f):
    global matrix
    global collected_strings
    global coll_section
    with open(DIR+"/"+f, "r") as fd:
        data = json.load(fd)
        x = [data['size'],
             data['bb'],
             # data['family'],
             data['call'],
             data['mov'],
             data['push'],
             data['pop'],
             data['add'],
             data['sub'],
             data['xor'],
             data['jmp']]

        for s in data['file_string']:
            if collected_strings.has_key(s) == False:
                collected_strings[s] = 1
            else:
                collected_strings[s] += 1

        for s in data['sections']:
            if collected_section.has_key(s) == False:
                collected_section[s] = 1
            else:
                collected_section[s] += 1

        if matrix == None:
            matrix = x
        else:
            if DEBUG:
                print "x = {}".format(x)
                print "Matrix \n\n {}".format(matrix)
            matrix = np.vstack([matrix, x])

"""
JSON db saver
f: file descriptor from open
"""
def storeJSON(f,i,matrix, coll_strings_dict, coll_section):
    with open(DIR+"/"+f, "r") as fd:
        with open(DIR_NORM + "/" + f, "w") as fd_out:
            try:
                data = json.load(fd)
                data['size'] = matrix[i][0]
                data['bb'] = matrix[i][1]
                # data['family'] = matrix[i][2]
                data['call'] = matrix[i][2]
                data['mov'] = matrix[i][3]
                data['push'] = matrix[i][4]
                data['pop'] = matrix[i][5]
                data['add'] = matrix[i][6]
                data['sub'] = matrix[i][7]
                data['xor'] = matrix[i][8]
                data['jmp'] = matrix[i][9]


                local_string_dict = dict.fromkeys(coll_strings_dict, 0)
                local_section_dict = dict.fromkeys(coll_section, 0)

                for s in data['file_string']:
                    if local_string_dict.has_key(s) != False:
                        local_string_dict[s] = 1

                for s in data['sections']:
                    if local_section_dict.has_key(s) != False:
                        local_section_dict[s] = 1

                data.update(local_string_dict)
                data.update(local_section_dict)
                local_string_dict = None
                local_section_dict = None
                data.pop('file_string', None)
                data.pop('sections', None)

                json.dump(data, fd_out, indent=4, separators=(',',':'), ensure_ascii=False)
            except:
                traceback.print_tb()
            return i


pool = Pool(processes=25)

"""
Function that performs the normalization of the features
fd: file descriptotr from open
"""
def normalize():
    global counter
    global matrix
    global collected_strings
    global coll_strings_dict
    global collected_section
    global coll_section
    global pool

    if DEBUG:
        print "{}".format(DIR)

    start = timeit.default_timer()

    for root, dirs, filenames in os.walk(DIR):
        # del filenames[100:]
        for f in filter(lambda x:x.endswith(".json"), filenames):
            counter = counter + 1
            loadJSON(f)
            bar.update(counter)

    coll_strings_dict = {k : v for k, v in collected_strings.iteritems() if v > 8}
    coll_section = {k : v for k, v in collected_section.iteritems() if v > 3}
    stop = timeit.default_timer()

    pprint (stop - start)

    print "Start Normalizing"
    matrix = preprocessing.normalize(matrix)
    print "Stop Normalizing"
    i = 0
    for root, dirs, filenames in os.walk(DIR):
        # del filenames[100:]
        for f in filter(lambda x:x.endswith(".json"), filenames):
            pool.apply_async(storeJSON, args=(f,i,matrix, coll_strings_dict,
                                              coll_section), callback=update)
            i += 1

    pool.close()
    pool.join()

    if DEBUG:
        print matrix
        print "Counter = {}".format(counter)

if __name__ == "__main__":
    bar = progressbar.ProgressBar(max_value=21736)
    normalize()
