import re
import os
import csv
import sys
import json
import string
import datetime
import progressbar

from multiprocessing import Pool

from malclass import element
from malclass import program_information

"""Debug log switch"""
DEBUG = False
"""String length sensibility"""
N = 6
"""Printable characters set"""
# printable = set(string.printable)
printable = [ c.encode('hex').upper() for c in string.printable.encode('ASCII')]
"""Times program turn 100"""
times = 0
"""Absolute DIR of the program"""
DIR = os.path.abspath(sys.argv[1])
"""Dictionary with all the features extracted"""
res = {}
"""Bar for status"""
bar = None
"""Path to training label"""
training_label = DIR+'/trainLabels.csv'
"""Getting parameters from command line"""
if len(sys.argv) > 2:
    outdir = os.path.abspath(sys.argv[2])
else:
    outdir= 'output'
"""Creating output directory (if needed)"""
if not os.path.exists(outdir):
    os.makedirs(outdir)

"""labels dictionary"""
labels = {}
"""Open and load cvs file"""
csv_file = open(training_label, 'rb')
reader = csv.reader(csv_file)

for row in reader:
    try:
        if DEBUG:
            print "key {}, Value {}".format(row[0], row[1])
        labels.update({row[0]:row[1]})
    except:
        print "Error"

"""Get size of the bytes file"""
def get_size(path_to_file):
    return os.stat(path_to_file).st_size

"""Get program information from asm"""

def get_program_info(path_to_file):
    ps = program_information()
    with open(path_to_file) as file_:
        subs = set()
        sections = set()
        ps.mov_count = 0
        ps.push_count = 0
        ps.pop_count = 0
        ps.add_count = 0
        ps.sub_count = 0
        ps.xor_count = 0
        ps.jmp_count = 0

        for line in file_:
            """Getting subs"""
            m = re.search(r'sub_\w+', line)
            if m != None:
                subs.add(m.group(0))

            """Getting sections"""
            m = re.search(r'\.\w+:', line)
            if m != None:
                sections.add(m.group(0))

            """Getting call"""
            m = re.search(r'call ', line)
            if m != None:
                ps.call_count = ps.call_count + 1

            """Getting mov"""
            m = re.search(r'mov', line)
            if m != None:
                ps.mov_count = ps.mov_count + 1

            """Getting push"""
            m = re.search(r'push', line)
            if m != None:
                ps.push_count = ps.push_count + 1

            """Getting push"""
            m = re.search(r'pop', line)
            if m != None:
                ps.pop_count = ps.pop_count + 1

            """Getting add"""
            m = re.search(r'add', line)
            if m != None:
                ps.add_count = ps.add_count + 1

            """Getting sub"""
            m = re.search(r'sub ',  line)
            if m != None:
                ps.sub_count = ps.sub_count + 1

            """Getting xor"""
            m = re.search(r'xor', line)
            if m != None:
                ps.xor_count = ps.xor_count + 1

            """Getting jmp"""
            m = re.search(r'j[a-zA-Z][a-zA-Z] ', line)
            if m != None:
                ps.jmp_count = ps.jmp_count + 1

    ps.bb_count = len(subs)
    ps.sections = list(sections)
    return ps

"""Get strings from bytes"""
def get_strings(path_to_file):
    el = []
    longline = ''
    bytes_ = []
    with open(path_to_file) as file_:
        for line in file_.readlines():
            longline += line[8:].replace(' ','').replace('\r\n','')

    #for byby in [longline[i:i+2] for i in range(0, len(longline), 2)]:
    for i in range(0, len(longline), 2):
        byby = longline[i:i+1]
        if byby in printable:
            bytes_.append(byby.decode('hex'))
        else:
            if not bytes_:
                continue
            else:
                string = "".join(bytes_)
                if len(string) > N:
                    el.append(string)
                bytes_ = []
    return el

"""Dump the data collected in json format"""
def dump_json(data):
    for key, el in data.iteritems():
        out = open(outdir+'/'+el.identifier+'.json', 'w')
        todump = {
                'id' : el.identifier,
                'asm_path' : el.file_asm,
                'bytes_path' : el.file_bytes,
                'file_string' : el.file_strings,
                'size' : el.file_size,
                'sections' : el.pi.sections,
                'bb': el.pi.bb_count,
                'family' : el.family,
                'call' : el.pi.call_count,
                'mov' : el.pi.mov_count,
                'push' : el.pi.push_count,
                'pop' : el.pi.pop_count,
                'add' : el.pi.add_count,
                'sub' : el.pi.sub_count,
                'xor' : el.pi.xor_count,
                'jmp' : el.pi.jmp_count,
                }
        json.dump(todump, out, indent=4, separators=(',',':'), ensure_ascii=False)
        out.close()

"""This function parallelize the feature extraction"""
def extract_features(absname, worker_id):
    obj = element(absname,
                  str(absname) + ".asm",
                  str(absname) + ".bytes")
    obj.file_size = get_size(os.path.join(DIR, obj.file_bytes))
    obj.pi = get_program_info(os.path.join(DIR, obj.file_asm))
    obj.file_strings = get_strings(os.path.join(DIR,obj.file_bytes))
    if obj.identifier in labels:
        obj.family = labels[obj.identifier]
    else:
        obj.family = -1

    if DEBUG:
        print "Worker {}) {}: {}".format(worker_id, absname, obj)
    return (worker_id,{absname:obj})

def collect_results(r):
    global times
    global bar
    worker_id, r = r
    if DEBUG:
        print "Collecting results for worker {}".format(worker_id)

    bar.update(times)
    times = times + 1

    if (not(len(res.keys()) % 500)):
        if DEBUG:
            print "{}: Collected {}".format(datetime.datetime.utcnow(),
                                        len(res.keys())*times)
        dump_json(res)
        res.clear()
    res.update(r)

"""Pool for multiprocessing"""
pool = Pool(processes=20)

for root, dirs, filenames in os.walk(DIR):
    i = 0
    bar = progressbar.ProgressBar(max_value=len(filenames)/2)
    for f in filter(lambda x:x.endswith(".bytes"), filenames):
        absname = f.split('.')[0]
        pool.apply_async(extract_features, args=(absname,i,),
                                         callback=collect_results)
        i = i+1
    pool.close()
    pool.join()
    for key,el in res.iteritems():
        print el.identifier
    dump_json(res)
