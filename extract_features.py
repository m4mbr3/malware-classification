import re
import os
import sys
import json
import string
import datetime
import progressbar

from multiprocessing import Pool

"""Debug log switch"""
DEBUG = False
"""String length sensibility"""
N = 6
"""Printable characters set"""
printable = set(string.printable)
"""Times program turn 100"""
times = 0
"""Absolute DIR of the program"""
DIR = os.path.abspath(sys.argv[1])
"""Dictionary with all the features extracted"""
res = {}
"""Bar for status"""
bar = None
"""Getting parameters from command line"""
if len(sys.argv) > 2:
    outfile = os.path.abspath(sys.argv[2])
else:
    outfile = 'output.txt'
"""Open output file"""
out = open(outfile, 'w')

"""Element to contain extracted information"""
class element(object):
    def __init__(self,
                 absname,
                 fileasm,
                 filebytes):
        """File ID"""
        self.identifier = absname
        """Path to the disassembled file"""
        self.file_asm = fileasm
        """Path to the bytes file"""
        self.file_bytes = filebytes
        """File size"""
        self.file_size = 0
        """Strings inside the file"""
        self.file_strings = []
        """Basic Block Counter"""
        self.file_bb = 0

"""Get size of the bytes file"""
def get_size(path_to_file):
    return os.stat(path_to_file).st_size

"""Get number of basic block"""
def get_bb_count(path_to_file):
    with open(path_to_file) as file_:
        subs = []
        for line in file_:
            m = re.search(r'sub_\w+',line)
            if m != None:
                subs.append(m.group(0))
    return len(set(subs))

"""Get strings from bytes"""
def get_strings(path_to_file):
    el = []
    with open(path_to_file) as file_:
        for line in file_:
            hex_bytes = line.strip().split()[1:]
            try:
                raw_bytes = map(lambda h_str: h_str.decode('hex'), hex_bytes)
                printable_bytes = filter(lambda s: 32 < ord(s) < 127, raw_bytes)

                if len(printable_bytes) > N:
                    el.append("".join(printable_bytes))
            except:
                continue
    return el

"""Dump the data collected in json format"""
def dump_json(data):
    for key, el in data.iteritems():
        todump = {
                'id' : el.identifier,
                'asm_path' : el.file_asm,
                'bytes_path' : el.file_bytes,
                'file_string' : el.file_strings,
                'size' : el.file_size,
                'bb': el.file_bb,
                }
        json.dump(todump, out, indent=4, separators=(',',':'), ensure_ascii=False)

"""This function parallelize the feature extraction"""
def extract_features(absname, worker_id):
    obj = element(absname,
                  str(absname) + ".asm",
                  str(absname) + ".bytes")
    obj.file_size = get_size(os.path.join(DIR, obj.file_bytes))
    obj.file_bb = get_bb_count(os.path.join(DIR, obj.file_asm))
    obj.file_strings = get_strings(os.path.join(DIR,obj.file_bytes))

    if DEBUG:
        print "Worker {}) {}: {}".format(worker_id, absname, obj)
    return (worker_id,{absname:obj})

def collect_results(r):
    global times
    global bar
    worker_id, r = r
    if DEBUG:
        print "Collecting results for worker {}".format(worker_id)

    bar.update(times)
    times = times + 1

    if (not(len(res.keys()) % 500)):
        if DEBUG:
            print "{}: Collected {}".format(datetime.datetime.utcnow(),
                                        len(res.keys())*times)
        dump_json(res)
        res.clear()
    res.update(r)

"""Pool for multiprocessing"""
pool = Pool(processes=32)

for root, dirs, filenames in os.walk(DIR):
    i = 0
    bar = progressbar.ProgressBar(max_value=len(filenames)/2)
    for f in filter(lambda x:x.endswith(".bytes"), filenames):
        absname = f.split('.')[0]
        pool.apply_async(extract_features, args=(absname,i,),
                                         callback=collect_results)
        i = i+1
    pool.close()
    pool.join()

    if DEBUG:
        print res
    dump_json(res)
    out.close()
